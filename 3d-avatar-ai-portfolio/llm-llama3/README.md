
# ğŸ¦™ LLM Agent - Llama 3.2 

Este proyecto proporciona un backend centrado en un agente LLM basado en Llama, que es el nÃºcleo de un portafolio personal interactivo. El objetivo principal es utilizar el agente LLM para responder preguntas tÃ©cnicas y profesionales basadas en la experiencia y el currÃ­culum de Cristian Vega SÃ¡nchez. 

## ğŸ“‹ DescripciÃ³n

El LLM responde de manera automÃ¡tica a preguntas complejas relacionadas con la experiencia profesional de Cristian, analizando el contexto y adaptando las respuestas a un tono profesional y tÃ©cnico. La integraciÃ³n del avatar 3D es una extensiÃ³n de este backend, enfocada en replicar la voz y apariencia del autor. AsÃ­ como tambien proporcionar amiaciones que se ejecutaran en su avatar 3D.

## ğŸ“‚ Estructura del Proyecto

```
ğŸ“¦ Avatar 3D Interactive Backend
 â”£ ğŸ“œ app.py                   # Script principal para iniciar el backend
 â”£ ğŸ“œ Dockerfile               # ConfiguraciÃ³n para la creaciÃ³n de contenedores Docker
 â”£ ğŸ“œ requirements.txt         # LibrerÃ­as y dependencias necesarias
 â”£ ğŸ“œ initial_prompt.txt       # Prompt inicial para personalizar respuestas del avatar
 â”— ğŸ“œ README.md                # Archivo de documentaciÃ³n actual
```

## ğŸ› ï¸ Setup e InstalaciÃ³n

1. Clona el repositorio:

   ```bash
   git clone https://github.com/budydeveloper/3d-avatar-ai-portfolio.git
   cd 3d-avatar-ai-portfolio/llm-llama3
   ```


2. ConfiguraciÃ³n del modelo Llama

[!IMPORTANT]  
Antes de ejecutar la aplicaciÃ³n, es necesario asegurarse de que el modelo Llama estÃ© instalado y en funcionamiento en el puerto `11434` de tu mÃ¡quina local.

1. **Descargar e instalar Ollama**:  
   Visita el siguiente enlace para descargar e instalar Ollama en tu sistema:  
   [Descargar Ollama](https://ollama.com/download)

2. **Ejecutar el modelo Llama**:  
   Inicia el modelo Llama con el siguiente comando desde la terminal:

   ```bash
   ollama run llama3.2
   ```

3. **Verificar la instalaciÃ³n**:  
   Puedes comprobar que Ollama se estÃ¡ ejecutando correctamente realizando una peticiÃ³n `curl` a `http://localhost:11434/`:

   ```bash
   curl http://localhost:11434/
   ```

   La respuesta esperada debe ser:

   ```json
   { "message": "Ollama is running" }
   ```



3. Usando Docker:

   ```bash
   docker build -t llm-agent-backend .
   docker run -p 5000:5000 llm-agent-backend
   ```

## ğŸ”¥ API Endpoints

### POST `/ask`

Este endpoint recibe preguntas y devuelve respuestas del avatar basadas en la informaciÃ³n proporcionada.

- **Request**:
  
  ```json
  {
      "question": "What technologies did you use at Dow Jones?"
  }
  ```

- **Response**:

  ```json
  {
      "animation": "nod",
      "response": "At Dow Jones, I primarily worked with AWS cloud services, Java, and various microservices components, handling data processing and analytics tasks."
  }
  ```

### Testing la API con `curl` en Windows CMD

Puedes probar el endpoint utilizando el siguiente comando desde la terminal de Windows:

```cmd
curl -X POST http://localhost:5000/ask -H "Content-Type: application/json" -d "{\"question\": \"What technologies did you use at Dow Jones?\"}"
```

## âš™ï¸ CustomizaciÃ³n

El comportamiento y las respuestas del avatar se pueden personalizar modificando el archivo `initial_prompt.txt`. Este archivo establece el tono y los temas permitidos para las respuestas. Cambia este archivo segÃºn tus necesidades antes de ejecutar la aplicaciÃ³n.

### Ejemplo de configuraciÃ³n:

El archivo `initial_prompt.txt` define la estructura de las respuestas. AsegÃºrate de que refleje tus preferencias.

## ğŸ“œ Licencia

Este proyecto se distribuye bajo la licencia MIT. Para mÃ¡s detalles, consulta el archivo `LICENSE` en el repositorio.

## ğŸ“§ Contacto

Si tienes alguna pregunta, duda o sugerencia, no dudes en contactar a **Cristian Vega SÃ¡nchez** a travÃ©s de [LinkedIn](https://www.linkedin.com/in/cristianve/) o enviar un correo a **budy.dev.tech@gmail.com**.
