
# ü¶ô LLM Agent - Llama 3.2 

Este proyecto proporciona un backend centrado en un agente LLM basado en Llama, que es el n√∫cleo de un portafolio personal interactivo. El objetivo principal es utilizar el agente LLM para responder preguntas t√©cnicas y profesionales basadas en la experiencia y el curr√≠culum de Cristian Vega S√°nchez. 

## üìã Descripci√≥n

El LLM responde de manera autom√°tica a preguntas complejas relacionadas con la experiencia profesional de Cristian, analizando el contexto y adaptando las respuestas a un tono profesional y t√©cnico. La integraci√≥n del avatar 3D es una extensi√≥n de este backend, enfocada en replicar la voz y apariencia del autor. As√≠ como tambien proporcionar amiaciones que se ejecutaran en su avatar 3D.

## üìÇ Estructura del Proyecto

```
üì¶ Avatar 3D Interactive Backend
 ‚î£ üìú app.py                   # Script principal para iniciar el backend
 ‚î£ üìú Dockerfile               # Configuraci√≥n para la creaci√≥n de contenedores Docker
 ‚î£ üìú requirements.txt         # Librer√≠as y dependencias necesarias
 ‚î£ üìú initial_prompt.txt       # Prompt inicial para personalizar respuestas del avatar
 ‚îó üìú README.md                # Archivo de documentaci√≥n actual
```

## üõ†Ô∏è Setup e Instalaci√≥n

1. Clona el repositorio:

   ```bash
   git clone https://github.com/budydeveloper/3d-avatar-ai-portfolio.git
   cd 3d-avatar-ai-portfolio/llm-llama3
   ```


2. Configuraci√≥n del modelo Llama

> [!IMPORTANT]
> Antes de ejecutar la aplicaci√≥n, es necesario asegurarse de que el modelo Llama est√© instalado y en funcionamiento en el puerto `11434` de tu m√°quina local.

1. **Descargar e instalar Ollama**:  
   Visita el siguiente enlace para descargar e instalar Ollama en tu sistema:  
   [Descargar Ollama](https://ollama.com/download)

2. **Ejecutar el modelo Llama**:  
   Inicia el modelo Llama con el siguiente comando desde la terminal:

   ```bash
   ollama run llama3.2
   ```

3. **Verificar la instalaci√≥n**:  
   Puedes comprobar que Ollama se est√° ejecutando correctamente realizando una petici√≥n `curl` a `http://localhost:11434/`:

   ```bash
   curl http://localhost:11434/
   ```

   La respuesta esperada debe ser:

   ```json
   { "message": "Ollama is running" }
   ```



3. Usando Docker:

   ```bash
   docker build -t llm-agent-backend .
   docker run -p 5000:5000 llm-agent-backend
   ```

## üî• API Endpoints

### POST `/ask`

Este endpoint recibe preguntas y devuelve respuestas del avatar basadas en la informaci√≥n proporcionada.

- **Request**:
  
  ```json
  {
      "question": "What technologies did you use at Dow Jones?"
  }
  ```

- **Response**:

  ```json
  {
      "animation": "nod",
      "response": "At Dow Jones, I primarily worked with AWS cloud services, Java, and various microservices components, handling data processing and analytics tasks."
  }
  ```

### Testing la API con `curl` en Windows CMD

Puedes probar el endpoint utilizando el siguiente comando desde la terminal de Windows:

```cmd
curl -X POST http://localhost:5000/ask -H "Content-Type: application/json" -d "{\"question\": \"What technologies did you use at Dow Jones?\"}"
```

## ‚öôÔ∏è Customizaci√≥n

El comportamiento y las respuestas del avatar se pueden personalizar modificando el archivo `initial_prompt.txt`. Este archivo establece el tono y los temas permitidos para las respuestas. Cambia este archivo seg√∫n tus necesidades antes de ejecutar la aplicaci√≥n.

### Ejemplo de configuraci√≥n:

El archivo `initial_prompt.txt` define la estructura de las respuestas. Aseg√∫rate de que refleje tus preferencias.

## üìú Licencia

Este proyecto se distribuye bajo la licencia MIT. Para m√°s detalles, consulta el archivo `LICENSE` en el repositorio.

## üìß Contacto

Si tienes alguna pregunta, duda o sugerencia, no dudes en contactar a **Cristian Vega S√°nchez** a trav√©s de [LinkedIn](https://www.linkedin.com/in/cristianve/) o enviar un correo a **budy.dev.tech@gmail.com**.
